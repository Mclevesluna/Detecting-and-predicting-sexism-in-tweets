{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049f31d7-9447-49b0-803f-971d511a8536",
   "metadata": {},
   "source": [
    "# Proyect Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e44138-320a-433d-ab6f-0f1ec1cf0f55",
   "metadata": {},
   "source": [
    "**1. Data exploration and processing**\n",
    "\n",
    "**2. Sexism detector:**\n",
    "\n",
    "    2.1 Dictionary-Based Sentiment Analysis to create sexism score\n",
    "    \n",
    "    2.2 Quicksort to organize data by sexism score\n",
    "    \n",
    "    2.3 Markov Chains for word predictions\n",
    "    \n",
    "    2.4 Algorithm for matrix multiplication (Strassen)\n",
    "   \n",
    "    2.5 Logistic regression with Strassen matrix multiplication and Gradient Descent\n",
    "\n",
    "    2.6 Co-ocurrence tree to analyze words commonly used together in sexist tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c96802-feea-47eb-bed5-cccde893a94c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ade5779-50c9-4eac-b007-20a8a7cf87e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mclevesluna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import the neccesary libraries\n",
    "# fasttext: commonly used for natural language processing tasks\n",
    "# io: for inout/output operations needed for interacting with data. Reading from files, writing to databases, and communicating with external systems\n",
    "# re: regular expressions are sequences of characters that define a search pattern. They are used for pattern matching within strings.\n",
    "# nltk: the \"stopwords\" module from the NLTK library provides a predefined list of stopwords for various languages including languages. This allows us to remove clutter words like \"and\", \"the\", etc.\n",
    "# PrettyTable: allows us to visualize the Markov Chain in a simple way\n",
    "\n",
    "import fasttext\n",
    "import io\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random \n",
    "import csv\n",
    "import sys\n",
    "\n",
    "#All of these were used for the logistic regression with matrix multiplication\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# #This library was ONLY used in step 2.5.1 (this is a bonus algorithm I included because it was interesting to me)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87cc4f87-d5af-4ef7-b9a3-f603e430e4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221aa621-f1f2-4d63-a69d-ac4e80aa9e9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Data exploration and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561f8b4e-9be4-4da4-825d-d0bef6b1599b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to read CSV file with error handling and a custom delimiter\n",
    "def read_csv_with_error_handling(file_path, delimiter=';'):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        reader = csv.reader(file, delimiter=delimiter)\n",
    "        for line_num, row in enumerate(reader, start=1):\n",
    "            try:\n",
    "                data.append(row)\n",
    "            except csv.Error as e:\n",
    "                print(f'Error at line {line_num}: {e}')\n",
    "    return data\n",
    "\n",
    "# Import training data with error handling and semicolon delimiter\n",
    "df_tra = pd.DataFrame(read_csv_with_error_handling(\"../Project/EXIST_2021_Dataset/training/EXIST2021_trainingFIXED.csv\"))\n",
    "df_training = df_tra.sample(frac=0.1, random_state=42)\n",
    "# Make sure that \"source\" and \"task1\" are read as strings\n",
    "df_training[2] = df_training[2].astype(str)\n",
    "df_training[5] = df_training[5].astype(str)\n",
    "\n",
    "# Testing data with error handling and semicolon delimiter\n",
    "df_tes = pd.DataFrame(read_csv_with_error_handling(\"../Project/EXIST_2021_Dataset/test/EXIST2021_test_labeledFIXED.csv\"))\n",
    "df_test = df_tes.sample(frac=0.1, random_state=42)\n",
    "# Make sure that \"source\" and \"task1\" are read as strings\n",
    "df_test[2] = df_test[2].astype(str)\n",
    "df_test[5] = df_test[5].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53f3340-4b83-459c-807e-a2331dce7841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0     1        2   3  \\\n",
      "2908  EXIST2021  2935  twitter  en   \n",
      "2666  EXIST2021  2693  twitter  en   \n",
      "5809  EXIST2021  5887  twitter  es   \n",
      "5832  EXIST2021  5910  twitter  es   \n",
      "3710  EXIST2021  3760  twitter  es   \n",
      "\n",
      "                                                      4           5  \\\n",
      "2908  Ex-#Cuomo Aide: He '#Sexually #Harassed Me for...      sexist   \n",
      "2666         @Cannedbirds I dont hit women but probably      sexist   \n",
      "5809  @ldpsincomplejos Va saliendo todo a la luz, la...      sexist   \n",
      "5832  Esta Claudia no es más que una lagartona que t...      sexist   \n",
      "3710  @Toni0084 abortar no es desear la muerte, nadi...  non-sexist   \n",
      "\n",
      "                                 6  \n",
      "2908               sexual-violence  \n",
      "2666  misogyny-non-sexual-violence  \n",
      "5809  misogyny-non-sexual-violence  \n",
      "5832  misogyny-non-sexual-violence  \n",
      "3710                    non-sexist  \n"
     ]
    }
   ],
   "source": [
    "print(df_training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d039ac35-1975-4ce2-9cc6-e4af47d25944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0      1        2   3  \\\n",
      "2347  EXIST2021   9324  twitter  es   \n",
      "2399  EXIST2021   9376  twitter  es   \n",
      "1564  EXIST2021   8541  twitter  en   \n",
      "3989  EXIST2021  10966  twitter  es   \n",
      "3279  EXIST2021  10256  twitter  es   \n",
      "\n",
      "                                                      4           5  \\\n",
      "2347  @anluma99 @abulelrafas Mal que lo hubiera hech...  non-sexist   \n",
      "2399  Me explicaron que cuando los hombres abren las...      sexist   \n",
      "1564  @olamiposiabeni @mobolajinafisa1 @FaisalokoMor...  non-sexist   \n",
      "3989  Eu segurando o choro quando o Chris canta ‚ÄúV...  non-sexist   \n",
      "3279  @maic00n__ Mitoooo #mgtow #gayscombolsonaro #lgbt  non-sexist   \n",
      "\n",
      "                           6  \n",
      "2347              non-sexist  \n",
      "2399  ideological-inequality  \n",
      "1564              non-sexist  \n",
      "3989              non-sexist  \n",
      "3279              non-sexist  \n"
     ]
    }
   ],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb33d0-c230-43bc-8d2a-077a1041e657",
   "metadata": {},
   "source": [
    "**For future reference:**\n",
    "\n",
    "2 = \"source\"\n",
    "3 = \"language\"\n",
    "4 = \"text\"\n",
    "5 = \"task1\"\n",
    "6 = \"task2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81e045c-47fc-4383-aa31-98e20efc90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean both spanish and english tweets (remove spaces, tags, links, make everything lowercase, remove stopwords with little value)\n",
    "\n",
    "def clean_text(text, language):\n",
    "    if language == \"en\":\n",
    "        # keep only words\n",
    "        remove_links = re.sub(r\"(https?\\://)\\S+\", \"link\", text)\n",
    "        remove_tags = re.sub(r\"(?:\\@)\\S+\", \"tag\", text)\n",
    "        letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", remove_links)\n",
    "        # convert to lower case and split \n",
    "        words = letters_only_text.lower().split()\n",
    "        # remove stopwords\n",
    "        stopword_set = set(stopwords.words(\"english\"))\n",
    "        meaningful_words = [w for w in words if w not in stopword_set]\n",
    "        # join the cleaned words in a list\n",
    "        return \" \".join(meaningful_words)\n",
    "     \n",
    "    #what do clean if its in spanish\n",
    "    else:        \n",
    "        # keep only words\n",
    "        remove_links = re.sub(r\"(https?\\://)\\S+\", \"link\", text)\n",
    "        remove_tags = re.sub(r\"(?:\\@)\\S+\", \"tag\", text)\n",
    "        letters_only_text = re.sub(\"[^abcdefghijklmnñopqrstuvwxyzABCDEFGHIJKLMNÑOPQRSTUVWXYZáéíóúüç]\", \" \", remove_links)\n",
    "        # convert to lower case and split \n",
    "        words = letters_only_text.lower().split()\n",
    "        # remove stopwords\n",
    "        stopword_set = set(stopwords.words(\"spanish\"))\n",
    "        meaningful_words = [w for w in words if w not in stopword_set]\n",
    "        # join the cleaned words in a list\n",
    "        return \" \".join(meaningful_words)\n",
    "\n",
    "# Create a new column for cleaned text in the training set bases off of current text (4) and language (3) rows\n",
    "df_training['clean_text'] = df_training.apply(lambda row: clean_text(row[4], row[3]), axis=1)\n",
    "\n",
    "# Create a new column for cleaned text in the test set\n",
    "df_test['clean_text'] = df_test.apply(lambda row: clean_text(row[4], row[3]), axis=1)\n",
    "\n",
    "#Make sure \"task1\" has no trailing or leading spaces\n",
    "df_training[5] = df_training[5].str.strip()\n",
    "df_test[5] = df_test[5].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e3c7a-12f5-46ba-8061-09f212bcf93e",
   "metadata": {},
   "source": [
    "# 2. Sexism detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7e3bf-0df6-45b0-b8e9-b0b4710d84ce",
   "metadata": {},
   "source": [
    "**Sentiment analysis to calculate a sexism score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c1125a-8bd1-4d22-b53e-6481b1100439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dictionary of sexist words using the task1 in our training set\n",
    "sexist_words = df_training[df_training[5] == 'sexist']['clean_text'].str.split().explode().tolist()\n",
    "df_sexist_dict = set(sexist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69abb548-d9de-41cb-89bc-c94c2843171d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create function to analyze and score sexism based on the number of sexist words per sexist tweet\n",
    "def sentiment_analysis(text, df_sexist_dict):\n",
    "     # Split the cleaned tweet into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Calculate the total sentiment score for the tweet\n",
    "    total_score = sum([word in df_sexist_dict for word in words])\n",
    "\n",
    "    # Define a threshold for classification (0 because we won't tolerate even 1 sexist word)\n",
    "    threshold = 0 \n",
    "\n",
    "    return total_score\n",
    "\n",
    "#The complecity for this algorithm is: O(N + M * N), where N is the number of words in the input text and M is the number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91255f54-b7bb-4eee-9f3c-0e30182c9364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sexism_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>9324</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>@anluma99 @abulelrafas Mal que lo hubiera hech...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>anluma abulelrafas mal hecho miembro partido j...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>9376</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>Me explicaron que cuando los hombres abren las...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "      <td>explicaron hombres abren piernas locomoci n p ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>8541</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@olamiposiabeni @mobolajinafisa1 @FaisalokoMor...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>olamiposiabeni mobolajinafisa faisalokomori ab...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>10966</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>Eu segurando o choro quando o Chris canta ‚ÄúV...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>eu segurando choro quando chris canta úvamo te...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3279</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>10256</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>@maic00n__ Mitoooo #mgtow #gayscombolsonaro #lgbt</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>maic n mitoooo mgtow gayscombolsonaro lgbt</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0      1        2   3  \\\n",
       "2347  EXIST2021   9324  twitter  es   \n",
       "2399  EXIST2021   9376  twitter  es   \n",
       "1564  EXIST2021   8541  twitter  en   \n",
       "3989  EXIST2021  10966  twitter  es   \n",
       "3279  EXIST2021  10256  twitter  es   \n",
       "\n",
       "                                                      4           5  \\\n",
       "2347  @anluma99 @abulelrafas Mal que lo hubiera hech...  non-sexist   \n",
       "2399  Me explicaron que cuando los hombres abren las...      sexist   \n",
       "1564  @olamiposiabeni @mobolajinafisa1 @FaisalokoMor...  non-sexist   \n",
       "3989  Eu segurando o choro quando o Chris canta ‚ÄúV...  non-sexist   \n",
       "3279  @maic00n__ Mitoooo #mgtow #gayscombolsonaro #lgbt  non-sexist   \n",
       "\n",
       "                           6  \\\n",
       "2347              non-sexist   \n",
       "2399  ideological-inequality   \n",
       "1564              non-sexist   \n",
       "3989              non-sexist   \n",
       "3279              non-sexist   \n",
       "\n",
       "                                             clean_text  sexism_score  \n",
       "2347  anluma abulelrafas mal hecho miembro partido j...            10  \n",
       "2399  explicaron hombres abren piernas locomoci n p ...             5  \n",
       "1564  olamiposiabeni mobolajinafisa faisalokomori ab...             6  \n",
       "3989  eu segurando choro quando chris canta úvamo te...             5  \n",
       "3279         maic n mitoooo mgtow gayscombolsonaro lgbt             2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply sentiment analysis to our training and test dataset and add as a column (we will use this column later for our logistic regression)\n",
    "df_training[\"sexism_score\"] = df_training[\"clean_text\"].apply(lambda x: sentiment_analysis(x, df_sexist_dict))\n",
    "df_training.head()\n",
    "\n",
    "df_test[\"sexism_score\"] = df_test[\"clean_text\"].apply(lambda x: sentiment_analysis(x, df_sexist_dict))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1df8f-65c9-4e66-b365-40528176f9a8",
   "metadata": {},
   "source": [
    "**Quicksort to organize by sexism score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3a5e8ad-e1e8-49c4-8aed-5e505200a70f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0     1        2   3  \\\n",
      "0    EXIST2021  4108  twitter  es   \n",
      "1    EXIST2021  1128  twitter  en   \n",
      "2    EXIST2021    95  twitter  en   \n",
      "3    EXIST2021   709  twitter  en   \n",
      "4    EXIST2021   406      gab  en   \n",
      "..         ...   ...      ...  ..   \n",
      "685  EXIST2021  1383  twitter  en   \n",
      "686  EXIST2021  6484  twitter  es   \n",
      "687  EXIST2021  2645  twitter  en   \n",
      "688  EXIST2021  1781  twitter  en   \n",
      "689  EXIST2021  2612  twitter  en   \n",
      "\n",
      "                                                     4           5  \\\n",
      "0    @soysi_tambien @gabrielboric Retaguardia Estrecha  non-sexist   \n",
      "1    this is my cockits harder than a rockhorny hou...  non-sexist   \n",
      "2       @holyquor @Answerforu2 #NotAllMen admit defeat  non-sexist   \n",
      "3                     @FaisalokoMori Bearded women nko  non-sexist   \n",
      "4    @jodecivante And I highly doubt you’re ugly be...  non-sexist   \n",
      "..                                                 ...         ...   \n",
      "685  these boys start dating one day &amp; expect t...      sexist   \n",
      "686  @ainhoaeus @BcnInsania @Eritacus @ilusocial @m...      sexist   \n",
      "687  @EXPELincels @beeonroids @shahjoffe @Ponderer_...  non-sexist   \n",
      "688  @NinjaSocialist @MgtowRadical @CrossBiddy @nat...  non-sexist   \n",
      "689  @CrossBiddy @NinjaSocialist @SR_Duncan @Shotgu...      sexist   \n",
      "\n",
      "                          6  \\\n",
      "0                non-sexist   \n",
      "1                non-sexist   \n",
      "2                non-sexist   \n",
      "3                non-sexist   \n",
      "4                non-sexist   \n",
      "..                      ...   \n",
      "685  stereotyping-dominance   \n",
      "686  stereotyping-dominance   \n",
      "687              non-sexist   \n",
      "688              non-sexist   \n",
      "689         sexual-violence   \n",
      "\n",
      "                                            clean_text  sexism_score  \n",
      "0      soysi tambien gabrielboric retaguardia estrecha             0  \n",
      "1    cockits harder rockhorny hours clockin bathroo...             0  \n",
      "2           holyquor answerforu notallmen admit defeat             0  \n",
      "3                      faisalokomori bearded women nko             1  \n",
      "4                 jodecivante highly doubt ugly bestie             1  \n",
      "..                                                 ...           ...  \n",
      "685  boys start dating one day amp expect partner l...            33  \n",
      "686  ainhoaeus bcninsania eritacus ilusocial mariam...            34  \n",
      "687  expelincels beeonroids shahjoffe ponderer purg...            56  \n",
      "688  ninjasocialist mgtowradical crossbiddy natspra...            57  \n",
      "689  crossbiddy ninjasocialist sr duncan shotgunrai...            86  \n",
      "\n",
      "[690 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Organize the data by the sexism score we created in our sentiment analysis algorithm\n",
    "\n",
    "def randomized_quicksort(data, column='sexism_score'):\n",
    "    if len(data) <= 1:\n",
    "        return data\n",
    "\n",
    "    # Randomly choose a pivot index\n",
    "    pivot_index = np.random.randint(0, len(data))\n",
    "    pivot = data[column].iloc[pivot_index]\n",
    "\n",
    "    # Split the DataFrame\n",
    "    less = data[data[column] < pivot]\n",
    "    equal = data[data[column] == pivot]\n",
    "    greater = data[data[column] > pivot]\n",
    "\n",
    "    # Sort the split data\n",
    "    return pd.concat([randomized_quicksort(less, column), equal, randomized_quicksort(greater, column)], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by the 'sexism_score' column\n",
    "sorted_dfTrain = randomized_quicksort(df_training, 'sexism_score')\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "print(sorted_dfTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e5b253-598e-40c0-a152-2f3219b27894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The complexity of this quicksort algorithm is: O(n log n), where 'n' is the number of tweets\n",
    "\n",
    "#Because we are selecting hte pivot randomnly, the worst case scenario is very unlikely and therefore not relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e76727-8152-4151-8725-ac7bec6f973f",
   "metadata": {},
   "source": [
    "**Markov chain to predict future words from user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7989bfc2-3547-4573-a8df-4d7aa984569d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We first need to tokenize our \"clean_text\" column to separate it into individual words\n",
    "tokenized_data = [clean_text(tweet, language) for tweet, language in zip(df_training[4], df_training[3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d1e993-ba6e-4c4b-8fff-5e39e5e7da53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Word                        Next Words\n",
      "0                 ex  [cuomo, reina, girlfriend, wife]\n",
      "1              cuomo                            [aide]\n",
      "2               aide                        [sexually]\n",
      "3           sexually                        [harassed]\n",
      "4           harassed             [years, theestallion]\n",
      "...              ...                               ...\n",
      "5773           lizzo                          [saying]\n",
      "5774           loose                          [weight]\n",
      "5775          weight                       [healthier]\n",
      "5776       healthier                         [somehow]\n",
      "5777  discriminating                             [fat]\n",
      "\n",
      "[5778 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build Markov Chain\n",
    "def build_markov_chain(inputtext):\n",
    "    chain = {}\n",
    "\n",
    "    for tweet in inputtext:\n",
    "        words = tweet.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            current_word = words[i]\n",
    "            next_word = words[i + 1]\n",
    "\n",
    "            if current_word in chain:\n",
    "                chain[current_word].append(next_word)\n",
    "            else:\n",
    "                chain[current_word] = [next_word]\n",
    "\n",
    "    return chain\n",
    "\n",
    "markov_chain = build_markov_chain(tokenized_data)\n",
    "\n",
    "# Convert Markov Chain to DataFrame\n",
    "df_markov_chain = pd.DataFrame(list(markov_chain.items()), columns=['Word', 'Next Words'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_markov_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd0da98-beab-452a-b695-7b938ea2a0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now, also using our training data, we will calculate the transition probabilites between words\n",
    "\n",
    "probabilities = {}\n",
    "def calculate_transition_probabilities(chain):\n",
    "    \n",
    "    for current_word, next_words in chain.items():\n",
    "        total_next_words = len(next_words)\n",
    "        probabilities[current_word] = {word: next_words.count(word) / total_next_words for word in set(next_words)}\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "transition_probabilities = calculate_transition_probabilities(markov_chain)\n",
    "\n",
    "# Convert probabilities to DataFrame\n",
    "df_transition_probabilities = pd.DataFrame(list(transition_probabilities.items()), columns=['Word', 'Next Words'])\n",
    "\n",
    "# Extract words and probabilities\n",
    "words = df_transition_probabilities['Word'].tolist()\n",
    "probabilities_matrix = df_transition_probabilities['Next Words'].apply(pd.Series).values\n",
    "\n",
    "# Assuming you have a list of words (cleaned_text)\n",
    "cleaned_text = \" \".join(df_training['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740bdf24-9c4c-4b5e-9d26-25f2f5cbd5c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80473"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184d5fbf-cf71-4297-9d7c-f6a830e07cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman bet would love white women sammy club dejan pueden ser jóvenes imprudentes creemos sabemos todas\n"
     ]
    }
   ],
   "source": [
    "#Next we have to generate the future sequences\n",
    "def generate_sequence(probabilities, seed, length=10):\n",
    "    current_word = seed\n",
    "    sequence = [current_word]\n",
    "       \n",
    "    for _ in range(length):\n",
    "        if current_word in probabilities.keys():\n",
    "            next_word = random.choices(list(probabilities[current_word].keys()), weights=list(probabilities[current_word].values()))[0]\n",
    "            sequence.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return ' '.join(sequence) \n",
    "   \n",
    "seed_word = \"woman\"\n",
    "generated_sequence = generate_sequence(probabilities, seed=seed_word, length=15)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea9acfb-d76a-4580-b353-5ca546a71cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Complexity of the full Markov chain:  O(N * M).The where N is the number of tweets and M is the average number of words in a tweet. Therefore, the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8df47-8d99-46a0-8ed3-bbf2af85433e",
   "metadata": {},
   "source": [
    "**Strassen's Algorithm for Matrix Multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8649b0c-ac29-49e0-bdbe-df9ae8ac3077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_matrix(matrix):\n",
    "    # Check if the matrix has only one dimension\n",
    "    if matrix.ndim == 1:\n",
    "        # If it's a 1D array, convert it to a 2D column vector\n",
    "        matrix = matrix.reshape((-1, 1))\n",
    "    \n",
    "    # the matrixes must be split into quadrants first\n",
    "    row, col = matrix.shape\n",
    "    row2, col2 = row // 2, col // 2\n",
    "    \n",
    "    upper_left = matrix[:row2, :col2]\n",
    "    upper_right = matrix[:row2, col2:]\n",
    "    lower_left = matrix[row2:, :col2]\n",
    "    lower_right = matrix[row2:, col2:]\n",
    "    \n",
    "    return upper_left, upper_right, lower_left, lower_right\n",
    "\n",
    "def strassen_multiply(A, B,threshold=50000):\n",
    "\n",
    "    # Base case: switch to a more efficient algorithm (e.g., NumPy)\n",
    "    if A.shape[0] <= threshold:\n",
    "        return np.dot(A, B)\n",
    "    \n",
    "    # Split matrices into four quadrants\n",
    "    a, b, c, d = split_matrix(A)\n",
    "    e, f, g, h = split_matrix(B)\n",
    "\n",
    "    # Recursive steps for Strassen's algorithm\n",
    "    p1 = strassen_multiply(a, f - h)\n",
    "    p2 = strassen_multiply(a + b, h)\n",
    "    p3 = strassen_multiply(c + d, e)\n",
    "    p4 = strassen_multiply(d, g - e)\n",
    "    p5 = strassen_multiply(a + d, e + h)\n",
    "    p6 = strassen_multiply(b - d, g + h)\n",
    "    p7 = strassen_multiply(a - c, e + f)\n",
    "\n",
    "    # Compute the quadrants of the result matrix\n",
    "    upper_left = p5 + p4 - p2 + p6\n",
    "    upper_right = p1 + p2\n",
    "    lower_left = p3 + p4\n",
    "    lower_right = p1 + p5 - p3 - p7\n",
    "\n",
    "    # Combine the quadrants to get the result matrix\n",
    "    result = np.vstack((np.hstack((upper_left, upper_right)),\n",
    "                        np.hstack((lower_left, lower_right))))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a1c8b1a-b20e-440f-b9ee-74bc52242e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_matrix:\n",
      "[[4 3]\n",
      " [5 2]]\n",
      "B\n",
      "[[5 1]\n",
      " [5 6]]\n",
      "\n",
      "Result of Matrix Multiplication:\n",
      "[[35 22]\n",
      " [35 17]]\n"
     ]
    }
   ],
   "source": [
    "#Create some quick test matrices to ensure our Strassen's Algorithm for Matrix Multiplication WORKS\n",
    "\n",
    "# Define two matrices A and B\n",
    "A = np.array([[4, 3], [5, 2]])\n",
    "B = np.array([[5, 1], [5, 6]])\n",
    "\n",
    "# Multiply matrices using Strassen's algorithm\n",
    "result = strassen_multiply(A,B)\n",
    "\n",
    "print(\"A_matrix:\")\n",
    "print(A)\n",
    "print(\"B\")\n",
    "print(B)\n",
    "print(\"\\nResult of Matrix Multiplication:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d0a650a-7969-4baf-a083-86a1a77f8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall time complexity of the algorithm: O(n**log(2⁡)7)=O(n**2.81), where n is the dimension in a n*n matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e59f6d-018f-489e-9575-c29fd9b0d88d",
   "metadata": {},
   "source": [
    "**Logistic Regression with Matrix Multiplication (Strassen) and Gradient Descent**\n",
    "\n",
    "In order to use two algorithms in one task, we will be doing a manual logistic regression with matrix multiplication. Our matrix multiplication will be conducted using the Strassen's Algorithm defined in the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c29d352b-6010-41c5-a225-b8eb68396085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908    twitter\n",
       "2666    twitter\n",
       "5809    twitter\n",
       "5832    twitter\n",
       "3710    twitter\n",
       "         ...   \n",
       "3257    twitter\n",
       "1599    twitter\n",
       "4536    twitter\n",
       "1009    twitter\n",
       "734         gab\n",
       "Name: 2, Length: 690, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28c5d9bd-fae5-4a77-be7c-0081b115fff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert binary categoric variables (sources and language) into numeric ones\n",
    "df_training['numeric_language']= df_training[3].apply(lambda x: 1 if x == 'en' else (2 if x == 'es' else 0))\n",
    "df_test['numeric_language']=df_test[3].apply(lambda x: 1 if x == 'en' else (2 if x == 'es' else 0))\n",
    "\n",
    "df_training['numeric_source']= df_training[2].apply(lambda x: 1 if x == 'twitter' else (2 if x == 'gab' else 0))\n",
    "df_test['numeric_source']=df_test[2].apply(lambda x: 1 if x == 'twitter' else (2 if x == 'gab' else 0))\n",
    "\n",
    "df_test['sexism_score'] = pd.to_numeric(df_test['sexism_score'], errors='coerce')\n",
    "df_training['sexism_score'] = pd.to_numeric(df_training['sexism_score'], errors='coerce')\n",
    "df_test['numeric_language'] = pd.to_numeric(df_test['numeric_language'], errors='coerce')\n",
    "df_training['numeric_language'] = pd.to_numeric(df_training['numeric_language'], errors='coerce')\n",
    "df_test['numeric_source'] = pd.to_numeric(df_test['numeric_source'], errors='coerce')\n",
    "df_training['numeric_source'] = pd.to_numeric(df_training['numeric_source'], errors='coerce')\n",
    "\n",
    "df_training[5]= df_training[5].apply(lambda x: 1 if x == 'sexist' else (0 if x == 'non-sexist' else 0))\n",
    "df_test[5]=df_test[5].apply(lambda x: 1 if x == 'sexist' else (0 if x == 'non-sexist' else 0))\n",
    "\n",
    "X = df_training[['numeric_source','numeric_language', 'sexism_score']]\n",
    "\n",
    "# Convert our sexism output into a NumPy array for matrix multiplication\n",
    "y = df_training[5].values  \n",
    "# Convert our variables array for matrix multiplication\n",
    "X_matrix = X.values\n",
    "\n",
    "#Gettting my aprameters ready\n",
    "# Initialize and transposing theta\n",
    "theta = np.zeros(X_matrix.shape[1])\n",
    "thetaT = np.transpose(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a97eeaa-3c56-45c3-8c29-3877d3a24b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the cost function\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(strassen_multiply(X, theta,threshold=50000))\n",
    "    \n",
    "    cost = y * np.log(h) + (1 - y) * np.log(1-h)\n",
    "    cost = -np.sum(cost)/m\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Define the gradient descent function\n",
    "def gradient_descent(X, y, theta, learning_rate, epochs):\n",
    "    m = len(y)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        h = sigmoid(strassen_multiply(X, theta,threshold=50000))\n",
    "        \n",
    "        gradient = (1/m) * strassen_multiply(X.T,(h - y),threshold=50000)\n",
    "        \n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "        cost = cost_function(X, y, theta)\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print(f'Epoch {epoch}, Cost: {cost}')\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1ba64d7-66c5-47d4-bf9b-86b35d394ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n",
      "1.0 2.0\n",
      "Epoch 0, Cost: 0.6584125201751524\n",
      "Epoch 2000, Cost: 0.5609166082909162\n",
      "Epoch 4000, Cost: 0.5519517053232531\n",
      "Epoch 6000, Cost: 0.5494692204507458\n",
      "Epoch 8000, Cost: 0.5486331269132615\n",
      "Epoch 10000, Cost: 0.548323119111504\n",
      "Epoch 12000, Cost: 0.548195805362609\n",
      "Epoch 14000, Cost: 0.5481367127968545\n",
      "Epoch 16000, Cost: 0.5481054932049796\n",
      "Epoch 18000, Cost: 0.548087041764228\n",
      "Epoch 20000, Cost: 0.5480752299946972\n",
      "Epoch 22000, Cost: 0.5480672895848695\n",
      "Epoch 24000, Cost: 0.5480618046599061\n",
      "Epoch 26000, Cost: 0.5480579614902177\n",
      "Epoch 28000, Cost: 0.548055249080892\n",
      "Epoch 30000, Cost: 0.5480533277982507\n",
      "Epoch 32000, Cost: 0.5480519644677279\n",
      "Epoch 34000, Cost: 0.5480509962167659\n",
      "Epoch 36000, Cost: 0.5480503082689888\n",
      "Epoch 38000, Cost: 0.5480498193829668\n",
      "Epoch 40000, Cost: 0.5480494719286608\n",
      "Epoch 42000, Cost: 0.5480492249818979\n",
      "Epoch 44000, Cost: 0.548049049466964\n",
      "Epoch 46000, Cost: 0.5480489247213733\n",
      "Epoch 48000, Cost: 0.5480488360599325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1776294 ],\n",
       "       [ 0.26705672],\n",
       "       [ 0.17877411],\n",
       "       [-1.98711691]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "learning_rate = 0.01\n",
    "epochs = 50000\n",
    "\n",
    "def normalize(X):\n",
    "    return (X-np.min(X))/(np.max(X)-np.min(X)+1e-6)\n",
    "\n",
    "X = np.float16(X_matrix)\n",
    "y = df_training[5].values  \n",
    "y = np.float16(y[:,np.newaxis])\n",
    "for i in range(2):\n",
    "    print(np.min(X[:,i]), np.max(X[:,i]))\n",
    "    X[:,i] = normalize(X[:,i])\n",
    "X = np.hstack((X, np.ones((X.shape[0], 1), dtype=X.dtype)))\n",
    "y = normalize(y)\n",
    "theta = np.zeros(X.shape[1])\n",
    "theta = theta[:,np.newaxis]\n",
    "\n",
    "theta_final = gradient_descent(X, y, theta, learning_rate, epochs)\n",
    "theta_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f83eee1e-47dd-41a7-b43f-9c8f8f40c73d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define test matrices\n",
    "y_test = df_test[5]\n",
    "y_test = np.array(y_test)\n",
    "X_test = df_test[['numeric_source','numeric_language','sexism_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc06b28-3a99-47c7-a91f-1477417565b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5480487730716307\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test set\n",
    "\n",
    "predictions = np.array(sigmoid(np.dot(X, theta_final)))\n",
    "y_test = df_test[5]\n",
    "\n",
    "# print(predictions[:,0])\n",
    "print(cost_function(X, y, theta_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acc29fde-5a82-40ff-9d9f-45bf8c3149ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36425949],\n",
       "       [0.25100747],\n",
       "       [0.94921657],\n",
       "       [0.26796661],\n",
       "       [0.6862784 ],\n",
       "       [0.60473387],\n",
       "       [0.42803636],\n",
       "       [0.92893993],\n",
       "       [0.98736114],\n",
       "       [0.4065732 ],\n",
       "       [0.16388946],\n",
       "       [0.42803636],\n",
       "       [0.20383235],\n",
       "       [0.30445136],\n",
       "       [0.30445136],\n",
       "       [0.70542544],\n",
       "       [0.23438079],\n",
       "       [0.89327964],\n",
       "       [0.28608555],\n",
       "       [0.62614658],\n",
       "       [0.34357218],\n",
       "       [0.77396156],\n",
       "       [0.21891158],\n",
       "       [0.15186341],\n",
       "       [0.95340552],\n",
       "       [0.93987339],\n",
       "       [0.98202635],\n",
       "       [0.53946088],\n",
       "       [0.32394556],\n",
       "       [0.9161956 ],\n",
       "       [0.90140822],\n",
       "       [0.99967268],\n",
       "       [0.4065732 ],\n",
       "       [0.51691378],\n",
       "       [0.23438079],\n",
       "       [0.56130378],\n",
       "       [0.26796661],\n",
       "       [0.4065732 ],\n",
       "       [0.75773942],\n",
       "       [0.64657166],\n",
       "       [0.51691378],\n",
       "       [0.92288707],\n",
       "       [0.26796661],\n",
       "       [0.58344862],\n",
       "       [0.830386  ],\n",
       "       [0.4065732 ],\n",
       "       [0.49484977],\n",
       "       [0.34357218],\n",
       "       [0.86476246],\n",
       "       [0.81725716],\n",
       "       [0.66696607],\n",
       "       [0.70542544],\n",
       "       [0.26796661],\n",
       "       [0.78903189],\n",
       "       [0.16388946],\n",
       "       [0.14083853],\n",
       "       [0.74116702],\n",
       "       [0.51691378],\n",
       "       [0.58344862],\n",
       "       [0.56130378],\n",
       "       [0.86476246],\n",
       "       [0.81725716],\n",
       "       [0.90140822],\n",
       "       [0.93468636],\n",
       "       [0.53946088],\n",
       "       [0.47225524],\n",
       "       [0.56102188],\n",
       "       [0.25100747],\n",
       "       [0.45032052],\n",
       "       [0.47225524],\n",
       "       [0.92288707],\n",
       "       [0.25100747],\n",
       "       [0.30445136],\n",
       "       [0.9161956 ],\n",
       "       [0.75773942],\n",
       "       [0.92893993],\n",
       "       [0.26796661],\n",
       "       [0.18987931],\n",
       "       [0.64657166],\n",
       "       [0.95717404],\n",
       "       [0.6862784 ],\n",
       "       [0.94921657],\n",
       "       [0.23438079],\n",
       "       [0.47225524],\n",
       "       [0.87500038],\n",
       "       [0.64657166],\n",
       "       [0.47225524],\n",
       "       [0.21891158],\n",
       "       [0.89327964],\n",
       "       [0.34357218],\n",
       "       [0.4065732 ],\n",
       "       [0.18987931],\n",
       "       [0.32394556],\n",
       "       [0.64657166],\n",
       "       [0.77396156],\n",
       "       [0.85410158],\n",
       "       [0.56130378],\n",
       "       [0.20383235],\n",
       "       [0.64657166],\n",
       "       [0.30445136],\n",
       "       [0.6862784 ],\n",
       "       [0.32369492],\n",
       "       [0.34357218],\n",
       "       [0.94921657],\n",
       "       [0.18987931],\n",
       "       [0.72343234],\n",
       "       [0.53946088],\n",
       "       [0.53946088],\n",
       "       [0.45032052],\n",
       "       [0.74116702],\n",
       "       [0.56130378],\n",
       "       [0.42803636],\n",
       "       [0.70542544],\n",
       "       [0.58344862],\n",
       "       [0.84246016],\n",
       "       [0.85410158],\n",
       "       [0.42803636],\n",
       "       [0.62614658],\n",
       "       [0.74116702],\n",
       "       [0.51691378],\n",
       "       [0.38493786],\n",
       "       [0.93468636],\n",
       "       [0.16388946],\n",
       "       [0.30445136],\n",
       "       [0.95717404],\n",
       "       [0.38493786],\n",
       "       [0.38466687],\n",
       "       [0.53946088],\n",
       "       [0.23438079],\n",
       "       [0.38493786],\n",
       "       [0.23438079],\n",
       "       [0.93468636],\n",
       "       [0.16388946],\n",
       "       [0.47225524],\n",
       "       [0.16388946],\n",
       "       [0.90906876],\n",
       "       [0.9161956 ],\n",
       "       [0.87500038],\n",
       "       [0.42803636],\n",
       "       [0.21891158],\n",
       "       [0.36425949],\n",
       "       [0.28608555],\n",
       "       [0.92893993],\n",
       "       [0.4065732 ],\n",
       "       [0.77396156],\n",
       "       [0.53946088],\n",
       "       [0.45032052],\n",
       "       [0.36425949],\n",
       "       [0.32394556],\n",
       "       [0.36425949],\n",
       "       [0.25079232],\n",
       "       [0.75773942],\n",
       "       [0.16373266],\n",
       "       [0.90916334],\n",
       "       [0.92893993],\n",
       "       [0.62614658],\n",
       "       [0.1763484 ],\n",
       "       [0.56130378],\n",
       "       [0.47225524],\n",
       "       [0.62614658],\n",
       "       [0.30445136],\n",
       "       [0.38493786],\n",
       "       [0.28608555],\n",
       "       [0.42803636],\n",
       "       [0.66696607],\n",
       "       [0.830386  ],\n",
       "       [0.28608555],\n",
       "       [0.4065732 ],\n",
       "       [0.42803636],\n",
       "       [0.36425949],\n",
       "       [0.36425949],\n",
       "       [0.45032052],\n",
       "       [0.25100747],\n",
       "       [0.56130378],\n",
       "       [0.70542544],\n",
       "       [0.14083853],\n",
       "       [0.70542544],\n",
       "       [0.62614658],\n",
       "       [0.36425949],\n",
       "       [0.16388946],\n",
       "       [0.72343234],\n",
       "       [0.26796661],\n",
       "       [0.51662793],\n",
       "       [0.40629704],\n",
       "       [0.4065732 ],\n",
       "       [0.4065732 ],\n",
       "       [0.26796661],\n",
       "       [0.9766267 ],\n",
       "       [0.14083853],\n",
       "       [0.95717404],\n",
       "       [0.72343234],\n",
       "       [0.28608555],\n",
       "       [0.66671176],\n",
       "       [0.58344862],\n",
       "       [0.85410158],\n",
       "       [0.6862784 ],\n",
       "       [0.38493786],\n",
       "       [0.16388946],\n",
       "       [0.30445136],\n",
       "       [0.830386  ],\n",
       "       [0.18987931],\n",
       "       [0.28608555],\n",
       "       [0.14083853],\n",
       "       [0.90916334],\n",
       "       [0.92288707],\n",
       "       [0.90916334],\n",
       "       [0.38493786],\n",
       "       [0.34357218],\n",
       "       [0.51691378],\n",
       "       [0.64657166],\n",
       "       [0.47225524],\n",
       "       [0.90916334],\n",
       "       [0.51662793],\n",
       "       [0.42803636],\n",
       "       [0.30445136],\n",
       "       [0.47225524],\n",
       "       [0.53946088],\n",
       "       [0.53946088],\n",
       "       [0.4065732 ],\n",
       "       [0.45032052],\n",
       "       [0.18987931],\n",
       "       [0.58344862],\n",
       "       [0.60473387],\n",
       "       [0.23438079],\n",
       "       [0.30445136],\n",
       "       [0.62614658],\n",
       "       [0.64657166],\n",
       "       [0.30445136],\n",
       "       [0.20383235],\n",
       "       [0.89327964],\n",
       "       [0.34357218],\n",
       "       [0.58344862],\n",
       "       [0.62614658],\n",
       "       [0.66671176],\n",
       "       [0.94921657],\n",
       "       [0.12056221],\n",
       "       [0.97449794],\n",
       "       [0.90140822],\n",
       "       [0.18987931],\n",
       "       [0.14083853],\n",
       "       [0.51691378],\n",
       "       [0.90140822],\n",
       "       [0.21871591],\n",
       "       [0.42803636],\n",
       "       [0.56130378],\n",
       "       [0.64657166],\n",
       "       [0.21891158],\n",
       "       [0.93468636],\n",
       "       [0.64657166],\n",
       "       [0.9161956 ],\n",
       "       [0.21891158],\n",
       "       [0.53946088],\n",
       "       [0.90916334],\n",
       "       [0.4065732 ],\n",
       "       [0.26796661],\n",
       "       [0.66671176],\n",
       "       [0.75773942],\n",
       "       [0.42803636],\n",
       "       [0.72343234],\n",
       "       [0.58344862],\n",
       "       [0.42803636],\n",
       "       [0.28608555],\n",
       "       [0.77396156],\n",
       "       [0.58344862],\n",
       "       [0.90916334],\n",
       "       [0.77396156],\n",
       "       [0.26796661],\n",
       "       [0.51691378],\n",
       "       [0.36425949],\n",
       "       [0.12056221],\n",
       "       [0.1763484 ],\n",
       "       [0.96393201],\n",
       "       [0.4065732 ],\n",
       "       [0.25100747],\n",
       "       [0.16388946],\n",
       "       [0.28608555],\n",
       "       [0.20383235],\n",
       "       [0.47225524],\n",
       "       [0.42803636],\n",
       "       [0.66696607],\n",
       "       [0.36425949],\n",
       "       [0.32394556],\n",
       "       [0.20383235],\n",
       "       [0.28608555],\n",
       "       [0.21891158],\n",
       "       [0.1763484 ],\n",
       "       [0.49484977],\n",
       "       [0.93987339],\n",
       "       [0.34357218],\n",
       "       [0.90140822],\n",
       "       [0.93987339],\n",
       "       [0.56130378],\n",
       "       [0.60473387],\n",
       "       [0.1763484 ],\n",
       "       [0.30445136],\n",
       "       [0.20383235],\n",
       "       [0.92288707],\n",
       "       [0.90916334],\n",
       "       [0.25100747],\n",
       "       [0.74094736],\n",
       "       [0.66696607],\n",
       "       [0.47225524],\n",
       "       [0.90916334],\n",
       "       [0.88434097],\n",
       "       [0.6862784 ],\n",
       "       [0.92893993],\n",
       "       [0.64657166],\n",
       "       [0.95340552],\n",
       "       [0.34357218],\n",
       "       [0.23438079],\n",
       "       [0.1763484 ],\n",
       "       [0.30420901],\n",
       "       [0.92893993],\n",
       "       [0.18987931],\n",
       "       [0.25100747],\n",
       "       [0.47225524],\n",
       "       [0.18987931],\n",
       "       [0.53946088],\n",
       "       [0.72343234],\n",
       "       [0.62614658],\n",
       "       [0.95717404],\n",
       "       [0.25100747],\n",
       "       [0.28608555],\n",
       "       [0.30445136],\n",
       "       [0.56130378],\n",
       "       [0.90916334],\n",
       "       [0.36425949],\n",
       "       [0.47225524],\n",
       "       [0.16388946],\n",
       "       [0.38493786],\n",
       "       [0.6862784 ],\n",
       "       [0.38493786],\n",
       "       [0.4065732 ],\n",
       "       [0.34357218],\n",
       "       [0.23438079],\n",
       "       [0.4065732 ],\n",
       "       [0.90140822],\n",
       "       [0.38493786],\n",
       "       [0.38493786],\n",
       "       [0.4065732 ],\n",
       "       [0.98039996],\n",
       "       [0.60473387],\n",
       "       [0.56130378],\n",
       "       [0.28585182],\n",
       "       [0.14083853],\n",
       "       [0.20383235],\n",
       "       [0.32394556],\n",
       "       [0.1763484 ],\n",
       "       [0.80370147],\n",
       "       [0.47225524],\n",
       "       [0.28608555],\n",
       "       [0.62614658],\n",
       "       [0.28608555],\n",
       "       [0.34357218],\n",
       "       [0.18987931],\n",
       "       [0.49456362],\n",
       "       [0.16388946],\n",
       "       [0.78903189],\n",
       "       [0.51691378],\n",
       "       [0.38493786],\n",
       "       [0.21891158],\n",
       "       [0.51662793],\n",
       "       [0.53946088],\n",
       "       [0.30445136],\n",
       "       [0.36425949],\n",
       "       [0.830386  ],\n",
       "       [0.21891158],\n",
       "       [0.25100747],\n",
       "       [0.18987931],\n",
       "       [0.32394556],\n",
       "       [0.84246016],\n",
       "       [0.84246016],\n",
       "       [0.53917647],\n",
       "       [0.96965737],\n",
       "       [0.30445136],\n",
       "       [0.32394556],\n",
       "       [0.30445136],\n",
       "       [0.51691378],\n",
       "       [0.45032052],\n",
       "       [0.34357218],\n",
       "       [0.74116702],\n",
       "       [0.14083853],\n",
       "       [0.49484977],\n",
       "       [0.62614658],\n",
       "       [0.28608555],\n",
       "       [0.92288707],\n",
       "       [0.51691378],\n",
       "       [0.49484977],\n",
       "       [0.88434097],\n",
       "       [0.14083853],\n",
       "       [0.38493786],\n",
       "       [0.30445136],\n",
       "       [0.26796661],\n",
       "       [0.32394556],\n",
       "       [0.16388946],\n",
       "       [0.18987931],\n",
       "       [0.42803636],\n",
       "       [0.90140822],\n",
       "       [0.28608555],\n",
       "       [0.42803636],\n",
       "       [0.88434097],\n",
       "       [0.28608555],\n",
       "       [0.51691378],\n",
       "       [0.4065732 ],\n",
       "       [0.42803636],\n",
       "       [0.62614658],\n",
       "       [0.47225524],\n",
       "       [0.51691378],\n",
       "       [0.26796661],\n",
       "       [0.94921657],\n",
       "       [0.74116702],\n",
       "       [0.42803636],\n",
       "       [0.23438079],\n",
       "       [0.20383235],\n",
       "       [0.18987931],\n",
       "       [0.70542544],\n",
       "       [0.93987339],\n",
       "       [0.21871591],\n",
       "       [0.23438079],\n",
       "       [0.90916334],\n",
       "       [0.36425949],\n",
       "       [0.4065732 ],\n",
       "       [0.90140822],\n",
       "       [0.42803636],\n",
       "       [0.66696607],\n",
       "       [0.51691378],\n",
       "       [0.30445136],\n",
       "       [0.32394556],\n",
       "       [0.74116702],\n",
       "       [0.51691378],\n",
       "       [0.4065732 ],\n",
       "       [0.77396156],\n",
       "       [0.85410158],\n",
       "       [0.20383235],\n",
       "       [0.25079232],\n",
       "       [0.62614658],\n",
       "       [0.36425949],\n",
       "       [0.21891158],\n",
       "       [0.25100747],\n",
       "       [0.53946088],\n",
       "       [0.98202635],\n",
       "       [0.56130378],\n",
       "       [0.36425949],\n",
       "       [0.70542544],\n",
       "       [0.62614658],\n",
       "       [0.42803636],\n",
       "       [0.32394556],\n",
       "       [0.36425949],\n",
       "       [0.42803636],\n",
       "       [0.56130378],\n",
       "       [0.93468636],\n",
       "       [0.75773942],\n",
       "       [0.26796661],\n",
       "       [0.25079232],\n",
       "       [0.62614658],\n",
       "       [0.4065732 ],\n",
       "       [0.86476246],\n",
       "       [0.64657166],\n",
       "       [0.58344862],\n",
       "       [0.75773942],\n",
       "       [0.64657166],\n",
       "       [0.96393201],\n",
       "       [0.62614658],\n",
       "       [0.53946088],\n",
       "       [0.20383235],\n",
       "       [0.36425949],\n",
       "       [0.9669494 ],\n",
       "       [0.36425949],\n",
       "       [0.42803636],\n",
       "       [0.26796661],\n",
       "       [0.23438079],\n",
       "       [0.74116702],\n",
       "       [0.28608555],\n",
       "       [0.1763484 ],\n",
       "       [0.78903189],\n",
       "       [0.88434097],\n",
       "       [0.21891158],\n",
       "       [0.87500038],\n",
       "       [0.47225524],\n",
       "       [0.42803636],\n",
       "       [0.42803636],\n",
       "       [0.38493786],\n",
       "       [0.21891158],\n",
       "       [0.14083853],\n",
       "       [0.38493786],\n",
       "       [0.38493786],\n",
       "       [0.95340552],\n",
       "       [0.88434097],\n",
       "       [0.14083853],\n",
       "       [0.14083853],\n",
       "       [0.81725716],\n",
       "       [0.830386  ],\n",
       "       [0.72343234],\n",
       "       [0.56130378],\n",
       "       [0.53917647],\n",
       "       [0.23438079],\n",
       "       [0.20383235],\n",
       "       [0.86476246],\n",
       "       [0.90140822],\n",
       "       [0.18987931],\n",
       "       [0.32394556],\n",
       "       [0.30445136],\n",
       "       [0.62614658],\n",
       "       [0.34357218],\n",
       "       [0.53946088],\n",
       "       [0.87500038],\n",
       "       [0.16388946],\n",
       "       [0.84246016],\n",
       "       [0.32394556],\n",
       "       [0.36425949],\n",
       "       [0.66696607],\n",
       "       [0.23438079],\n",
       "       [0.92893993],\n",
       "       [0.47225524],\n",
       "       [0.47225524],\n",
       "       [0.99972625],\n",
       "       [0.89327964],\n",
       "       [0.53946088],\n",
       "       [0.16388946],\n",
       "       [0.23438079],\n",
       "       [0.25079232],\n",
       "       [0.25100747],\n",
       "       [0.20383235],\n",
       "       [0.45032052],\n",
       "       [0.16388946],\n",
       "       [0.28608555],\n",
       "       [0.42803636],\n",
       "       [0.83022471],\n",
       "       [0.9447882 ],\n",
       "       [0.16388946],\n",
       "       [0.36425949],\n",
       "       [0.26796661],\n",
       "       [0.84246016],\n",
       "       [0.42803636],\n",
       "       [0.62614658],\n",
       "       [0.42803636],\n",
       "       [0.51691378],\n",
       "       [0.93468636],\n",
       "       [0.53946088],\n",
       "       [0.34357218],\n",
       "       [0.66696607],\n",
       "       [0.28608555],\n",
       "       [0.62614658],\n",
       "       [0.45032052],\n",
       "       [0.32394556],\n",
       "       [0.84246016],\n",
       "       [0.72343234],\n",
       "       [0.16388946],\n",
       "       [0.84246016],\n",
       "       [0.38493786],\n",
       "       [0.92288707],\n",
       "       [0.42803636],\n",
       "       [0.34357218],\n",
       "       [0.58344862],\n",
       "       [0.78903189],\n",
       "       [0.38493786],\n",
       "       [0.62614658],\n",
       "       [0.26796661],\n",
       "       [0.30445136],\n",
       "       [0.51691378],\n",
       "       [0.45032052],\n",
       "       [0.70542544],\n",
       "       [0.34357218],\n",
       "       [0.92288707],\n",
       "       [0.93987339],\n",
       "       [0.58344862],\n",
       "       [0.6862784 ],\n",
       "       [0.38493786],\n",
       "       [0.89327964],\n",
       "       [0.32369492],\n",
       "       [0.45032052],\n",
       "       [0.80370147],\n",
       "       [0.64657166],\n",
       "       [0.85410158],\n",
       "       [0.53946088],\n",
       "       [0.25100747],\n",
       "       [0.47225524],\n",
       "       [0.30445136],\n",
       "       [0.81725716],\n",
       "       [0.4065732 ],\n",
       "       [0.88434097],\n",
       "       [0.85410158],\n",
       "       [0.42803636],\n",
       "       [0.60473387],\n",
       "       [0.49484977],\n",
       "       [0.81725716],\n",
       "       [0.26796661],\n",
       "       [0.26796661],\n",
       "       [0.93468636],\n",
       "       [0.38493786],\n",
       "       [0.66696607],\n",
       "       [0.1763484 ],\n",
       "       [0.56130378],\n",
       "       [0.830386  ],\n",
       "       [0.74116702],\n",
       "       [0.96965737],\n",
       "       [0.47225524],\n",
       "       [0.34357218],\n",
       "       [0.95717404],\n",
       "       [0.42803636],\n",
       "       [0.66696607],\n",
       "       [0.36425949],\n",
       "       [0.26796661],\n",
       "       [0.23438079],\n",
       "       [0.45032052],\n",
       "       [0.25100747],\n",
       "       [0.25100747],\n",
       "       [0.64657166],\n",
       "       [0.95717404],\n",
       "       [0.51691378],\n",
       "       [0.30445136],\n",
       "       [0.62614658],\n",
       "       [0.9161956 ],\n",
       "       [0.42803636],\n",
       "       [0.89327964],\n",
       "       [0.96069056],\n",
       "       [0.53946088],\n",
       "       [0.99999847],\n",
       "       [0.66696607],\n",
       "       [0.1763484 ],\n",
       "       [0.75773942],\n",
       "       [0.26796661],\n",
       "       [0.26796661],\n",
       "       [0.78903189],\n",
       "       [0.45032052],\n",
       "       [0.32394556],\n",
       "       [0.26796661],\n",
       "       [0.92288707],\n",
       "       [0.42803636],\n",
       "       [0.60473387],\n",
       "       [0.16388946],\n",
       "       [0.34357218],\n",
       "       [0.9722096 ],\n",
       "       [0.30445136],\n",
       "       [0.51691378],\n",
       "       [0.70542544],\n",
       "       [0.34357218],\n",
       "       [0.21891158],\n",
       "       [0.97449794],\n",
       "       [0.32394556],\n",
       "       [0.93980867],\n",
       "       [0.32394556],\n",
       "       [0.58344862],\n",
       "       [0.53946088],\n",
       "       [0.51691378],\n",
       "       [0.90140822],\n",
       "       [0.28608555],\n",
       "       [0.20383235],\n",
       "       [0.96393201],\n",
       "       [0.25100747],\n",
       "       [0.87500038],\n",
       "       [0.18987931],\n",
       "       [0.4065732 ],\n",
       "       [0.72343234],\n",
       "       [0.75773942],\n",
       "       [0.38493786],\n",
       "       [0.26796661],\n",
       "       [0.30445136],\n",
       "       [0.32394556],\n",
       "       [0.34357218],\n",
       "       [0.30445136],\n",
       "       [0.77396156],\n",
       "       [0.66696607],\n",
       "       [0.20383235],\n",
       "       [0.34357218],\n",
       "       [0.83022471],\n",
       "       [0.25100747],\n",
       "       [0.16388946],\n",
       "       [0.9766267 ],\n",
       "       [0.28608555],\n",
       "       [0.30445136],\n",
       "       [0.42803636],\n",
       "       [0.40629704],\n",
       "       [0.28608555],\n",
       "       [0.23438079],\n",
       "       [0.36425949],\n",
       "       [0.34357218],\n",
       "       [0.58344862],\n",
       "       [0.75773942],\n",
       "       [0.4065732 ],\n",
       "       [0.18987931],\n",
       "       [0.90916334],\n",
       "       [0.58317039],\n",
       "       [0.32394556],\n",
       "       [0.38493786],\n",
       "       [0.58344862],\n",
       "       [0.97665281],\n",
       "       [0.38493786],\n",
       "       [0.18987931],\n",
       "       [0.49456362]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "050de063-82f0-45f0-971f-2376dc2ce6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The complexity of this algorithm depends mostly on the Gradient Descent and the Matrix Multiplication:\n",
    "#Complexity: O(Matrix Multiplication+Gradient Descent)=O(n**2.81)+k⋅n⋅(d+1))\n",
    "#Where k is the number of epochs, n is the number of instances, and d is the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b023cae-adb0-465b-b482-395482071427",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Creating a co-ocurrences tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "468d1520-b83f-44d7-aaae-16597694d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, this is the sexist word dictionary we created in our sentiment analysis algorithm: df_sexist_dict\n",
    "# Remember, we had tokenized our tweet data to create our Markov Chains: tokenized_data \n",
    "\n",
    "#Convert our df_sexist_dict into list\n",
    "sexist_words = list(df_sexist_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fb9afc2-1098-458d-a1fa-32399c681a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      4  \\\n",
      "4291  Al guionista de Superl√≥pez no le da la puta g...   \n",
      "1886  @xlizagx I will take the handles hun.. serious...   \n",
      "3364  Michelle Bachelet reconoce violaci√≥n de derec...   \n",
      "4368  @elmundoes A Pablo es que ya no le hacen caso ...   \n",
      "4347  @vania_vargas Concuerdo... Mi hija y el resto ...   \n",
      "...                                                 ...   \n",
      "1904  Best explanation I‚Äôve seen of whole Google s...   \n",
      "1338  @HayliNic Grown ass woman, playing video games...   \n",
      "903   @CatfishKristen6 wields a psychosexual power o...   \n",
      "2252  @Berro_con_limon @MarioPscherer As√≠ son los m...   \n",
      "893   I hate when bitches say they gone beat my ass ...   \n",
      "\n",
      "                                         tokenized_text  \n",
      "4291  guionista superl pez da puta gana rodar blas l...  \n",
      "1886  xlizagx i will take the handles hun seriously ...  \n",
      "3364  michelle bachelet reconoce violaci n derechos ...  \n",
      "4368  elmundoes pablo hacen caso consejos soltar ton...  \n",
      "4347  vania vargas concuerdo hija resto mujeres debe...  \n",
      "...                                                 ...  \n",
      "1904  best explanation i ve seen of whole google sag...  \n",
      "1338  haylinic grown ass woman playing video games t...  \n",
      "903   catfishkristen wields psychosexual power over ...  \n",
      "2252  berro limon mariopscherer as machistas mujer c...  \n",
      "893   i hate when bitches say they gone beat my ass ...  \n",
      "\n",
      "[99 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/24m71qvx7n14q697ws0scfb40000gn/T/ipykernel_6557/1946314466.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  high_sexism_tweets['tokenized_text'] = high_sexism_tweets[4].astype(str).apply(clean_text, language=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to -- women: 62\n",
      "women -- to: 62\n",
      "t -- i: 60\n",
      "i -- t: 60\n",
      "to -- s: 49\n",
      "s -- to: 49\n",
      "the -- women: 43\n",
      "women -- the: 43\n",
      "s -- s: 42\n",
      "the -- in: 42\n",
      "in -- the: 42\n",
      "i -- hate: 40\n",
      "hate -- i: 40\n",
      "i -- in: 37\n",
      "the -- t: 37\n",
      "in -- i: 37\n",
      "t -- the: 37\n",
      "i -- women: 36\n",
      "men -- women: 36\n",
      "women -- i: 36\n",
      "women -- men: 36\n",
      "s -- the: 36\n",
      "the -- s: 36\n",
      "to -- men: 36\n",
      "men -- to: 36\n",
      "like -- i: 35\n",
      "i -- like: 35\n",
      "is -- s: 35\n",
      "s -- is: 35\n",
      "you -- t: 34\n",
      "t -- you: 34\n",
      "i -- s: 34\n",
      "is -- women: 34\n",
      "women -- is: 34\n",
      "s -- i: 34\n",
      "t -- it: 33\n",
      "it -- t: 33\n",
      "i -- at: 33\n",
      "at -- i: 33\n",
      "of -- women: 33\n",
      "women -- of: 33\n",
      "to -- t: 32\n",
      "to -- in: 32\n",
      "t -- to: 32\n",
      "in -- to: 32\n",
      "like -- to: 32\n",
      "to -- like: 32\n",
      "men -- men: 32\n",
      "i -- love: 32\n",
      "love -- i: 32\n"
     ]
    }
   ],
   "source": [
    "# Set a threshold for the sexism score\n",
    "threshold = 11  # This corresponds to the mean sexism score in our test data\n",
    "\n",
    "# Filter tweets based on the threshold\n",
    "high_sexism_tweets = df_test[df_test['sexism_score'] > threshold]\n",
    "\n",
    "# Convert the 'text' column to strings\n",
    "high_sexism_tweets['tokenized_text'] = high_sexism_tweets[4].astype(str).apply(clean_text, language=2)\n",
    "\n",
    "# Display the tokenized text\n",
    "print(high_sexism_tweets[[4, 'tokenized_text']])\n",
    "\n",
    "# Tokenize the text into words for high sexism tweets\n",
    "tokenized_high_sexism_tweets = [text.split() for text in high_sexism_tweets['tokenized_text']]\n",
    "\n",
    "# Create a co-occurrence matrix considering only sexist words in high sexism tweets\n",
    "co_occurrence_matrix = {}\n",
    "for words in tokenized_high_sexism_tweets:\n",
    "    for i, word_i in enumerate(words):\n",
    "        for j, word_j in enumerate(words):\n",
    "            if i != j and (word_i in sexist_words or word_j in sexist_words):\n",
    "                key = (word_i, word_j)\n",
    "                co_occurrence_matrix[key] = co_occurrence_matrix.get(key, 0) + 1\n",
    "\n",
    "# Print the 50 pairs with the highest weight (most common word combinations in sexist tweet)\n",
    "top_edges = sorted(co_occurrence_matrix.items(), key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "for edge, weight in top_edges:\n",
    "    print(f\"{edge[0]} -- {edge[1]}: {weight}\")  \n",
    "\n",
    "\n",
    "#You can activate this part to visualize the network, but it will print out a REALLY long list\n",
    "\n",
    "# for edge, weight in co_occurrence_matrix.items():\n",
    "#     print(f\"{edge[0]} -- {edge[1]}: {weight}\")\n",
    "\n",
    "# The warning that appears is NOT critical or relevant so we will ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933c0ef-d531-41bd-a833-c590b4dcac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdbfdbd-c3b5-4d2a-bc98-c4cfdffea384",
   "metadata": {},
   "source": [
    "**For a later time, we will finish constructing the following:**\n",
    "\n",
    "Predicting wether the string of words generated by our Markov Chain will be sexist according to our logistic regression**\n",
    "\n",
    "1. Preprocess the Generated String:\n",
    "Tokenize the generated string into words.\n",
    "2. Apply any necessary cleaning or preprocessing steps that you used during the training of your logistic regression model.\n",
    "3. Feature Extraction: Extract the same features from the generated string that were used as input features during the training of your logistic regression model. This may include word frequencies, presence of specific words, or any other relevant features.\n",
    "4. Use Logistic Regression Model:Input the extracted features into your trained logistic regression model to obtain a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1acbfe31-88ce-4554-afd1-fc1cab7bb597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Preprocess the string generated by the Markov Chain\n",
    "# tokenized_text = generated_sequence.split()\n",
    "# processed_generated_sequence = \" \".join(tokenized_text)\n",
    "\n",
    "# # Convert X_matrix to a list of strings\n",
    "# X_texts = [\" \".join(map(str, row)) for row in X_matrix]\n",
    "\n",
    "# # Fit and transform the processed text \n",
    "# tfidf_vectorizer.fit(X_texts)\n",
    "# generated_features = tfidf_vectorizer.transform([processed_generated_sequence])\n",
    "\n",
    "# # Add a constant term for bias to the features\n",
    "# generated_features_with_bias = np.hstack([generated_features.toarray(), np.ones((generated_features.shape[0], 1))])\n",
    "\n",
    "# # Assuming you have three features (numeric_source, numeric_language, sexism_score)\n",
    "# num_features = 3\n",
    "\n",
    "# # Ensure that the number of features matches the size of theta_final\n",
    "# assert generated_features_with_bias.shape[1] == num_features + 1, \"Number of features does not match theta_final size\"\n",
    "\n",
    "# # Now, make predictions using the logistic regression model\n",
    "# generated_predictions = sigmoid(np.dot(generated_features_with_bias, theta_final))\n",
    "\n",
    "# # Print the predictions\n",
    "# print(\"Generated Predictions:\", generated_predictions)\n",
    "\n",
    "# # Print the feature names (words)\n",
    "# print(\"Feature names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "# # Print the TF-IDF matrix\n",
    "# print(\"TF-IDF matrix:\\n\", generated_features.toarray())\n",
    "\n",
    "# Make predictions on the TF-IDF features\n",
    "#predictions = predictions \n",
    "\n",
    "# Print the predictions\n",
    "#print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
